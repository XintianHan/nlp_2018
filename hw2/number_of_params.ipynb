{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder:  rnn\n",
      "hidden_size :  300\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  True\n",
      "number of parameters:  1175103\n",
      "encoder:  cnn\n",
      "hidden_size :  300\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  True\n",
      "number of parameters:  632103\n",
      "encoder:  cnn\n",
      "hidden_size :  300\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  5\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  True\n",
      "number of parameters:  992103\n",
      "encoder:  rnn\n",
      "hidden_size :  400\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  True\n",
      "number of parameters:  1806303\n",
      "encoder:  cnn\n",
      "hidden_size :  400\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  True\n",
      "number of parameters:  962303\n",
      "encoder:  cnn\n",
      "hidden_size :  400\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  5\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  True\n",
      "number of parameters:  1522303\n",
      "encoder:  rnn\n",
      "hidden_size :  500\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  True\n",
      "number of parameters:  2557503\n",
      "encoder:  cnn\n",
      "hidden_size :  500\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  True\n",
      "number of parameters:  1352503\n",
      "encoder:  cnn\n",
      "hidden_size :  500\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  5\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  True\n",
      "number of parameters:  2152503\n",
      "encoder:  rnn\n",
      "hidden_size :  300\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  True\n",
      "number of parameters:  1265103\n",
      "encoder:  cnn\n",
      "hidden_size :  300\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  True\n",
      "number of parameters:  722103\n",
      "encoder:  cnn\n",
      "hidden_size :  300\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  5\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  True\n",
      "number of parameters:  1082103\n",
      "encoder:  rnn\n",
      "hidden_size :  400\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  True\n",
      "number of parameters:  1926303\n",
      "encoder:  cnn\n",
      "hidden_size :  400\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  True\n",
      "number of parameters:  1082303\n",
      "encoder:  cnn\n",
      "hidden_size :  400\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  5\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  True\n",
      "number of parameters:  1642303\n",
      "encoder:  rnn\n",
      "hidden_size :  500\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  True\n",
      "number of parameters:  2707503\n",
      "encoder:  cnn\n",
      "hidden_size :  500\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  True\n",
      "number of parameters:  1502503\n",
      "encoder:  cnn\n",
      "hidden_size :  500\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  5\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  True\n",
      "number of parameters:  2302503\n",
      "encoder:  rnn\n",
      "hidden_size :  300\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  False\n",
      "number of parameters:  1175103\n",
      "encoder:  cnn\n",
      "hidden_size :  300\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  False\n",
      "number of parameters:  632103\n",
      "encoder:  cnn\n",
      "hidden_size :  300\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  5\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  False\n",
      "number of parameters:  992103\n",
      "encoder:  rnn\n",
      "hidden_size :  400\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  False\n",
      "number of parameters:  1806303\n",
      "encoder:  cnn\n",
      "hidden_size :  400\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  False\n",
      "number of parameters:  962303\n",
      "encoder:  cnn\n",
      "hidden_size :  400\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  5\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  False\n",
      "number of parameters:  1522303\n",
      "encoder:  rnn\n",
      "hidden_size :  500\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  False\n",
      "number of parameters:  2557503\n",
      "encoder:  cnn\n",
      "hidden_size :  500\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  False\n",
      "number of parameters:  1352503\n",
      "encoder:  cnn\n",
      "hidden_size :  500\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  5\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  False\n",
      "number of parameters:  2152503\n",
      "encoder:  rnn\n",
      "hidden_size :  300\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  False\n",
      "number of parameters:  1265103\n",
      "encoder:  cnn\n",
      "hidden_size :  300\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  False\n",
      "number of parameters:  722103\n",
      "encoder:  cnn\n",
      "hidden_size :  300\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  5\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  False\n",
      "number of parameters:  1082103\n",
      "encoder:  rnn\n",
      "hidden_size :  400\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  False\n",
      "number of parameters:  1926303\n",
      "encoder:  cnn\n",
      "hidden_size :  400\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  False\n",
      "number of parameters:  1082303\n",
      "encoder:  cnn\n",
      "hidden_size :  400\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  5\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  False\n",
      "number of parameters:  1642303\n",
      "encoder:  rnn\n",
      "hidden_size :  500\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  False\n",
      "number of parameters:  2707503\n",
      "encoder:  cnn\n",
      "hidden_size :  500\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  False\n",
      "number of parameters:  1502503\n",
      "encoder:  cnn\n",
      "hidden_size :  500\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  5\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  False\n",
      "dropout? :  False\n",
      "number of parameters:  2302503\n",
      "encoder:  rnn\n",
      "hidden_size :  300\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  True\n",
      "number of parameters:  1175103\n",
      "encoder:  cnn\n",
      "hidden_size :  300\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  True\n",
      "number of parameters:  632103\n",
      "encoder:  cnn\n",
      "hidden_size :  300\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  5\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  True\n",
      "number of parameters:  992103\n",
      "encoder:  rnn\n",
      "hidden_size :  400\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  True\n",
      "number of parameters:  1806303\n",
      "encoder:  cnn\n",
      "hidden_size :  400\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  True\n",
      "number of parameters:  962303\n",
      "encoder:  cnn\n",
      "hidden_size :  400\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  5\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  True\n",
      "number of parameters:  1522303\n",
      "encoder:  rnn\n",
      "hidden_size :  500\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  True\n",
      "number of parameters:  2557503\n",
      "encoder:  cnn\n",
      "hidden_size :  500\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  True\n",
      "number of parameters:  1352503\n",
      "encoder:  cnn\n",
      "hidden_size :  500\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  5\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  True\n",
      "number of parameters:  2152503\n",
      "encoder:  rnn\n",
      "hidden_size :  300\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  True\n",
      "number of parameters:  1265103\n",
      "encoder:  cnn\n",
      "hidden_size :  300\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  True\n",
      "number of parameters:  722103\n",
      "encoder:  cnn\n",
      "hidden_size :  300\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  5\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  True\n",
      "number of parameters:  1082103\n",
      "encoder:  rnn\n",
      "hidden_size :  400\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  True\n",
      "number of parameters:  1926303\n",
      "encoder:  cnn\n",
      "hidden_size :  400\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  True\n",
      "number of parameters:  1082303\n",
      "encoder:  cnn\n",
      "hidden_size :  400\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  5\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  True\n",
      "number of parameters:  1642303\n",
      "encoder:  rnn\n",
      "hidden_size :  500\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  True\n",
      "number of parameters:  2707503\n",
      "encoder:  cnn\n",
      "hidden_size :  500\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  True\n",
      "number of parameters:  1502503\n",
      "encoder:  cnn\n",
      "hidden_size :  500\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  5\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  True\n",
      "number of parameters:  2302503\n",
      "encoder:  rnn\n",
      "hidden_size :  300\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  False\n",
      "number of parameters:  1175103\n",
      "encoder:  cnn\n",
      "hidden_size :  300\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  False\n",
      "number of parameters:  632103\n",
      "encoder:  cnn\n",
      "hidden_size :  300\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  5\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  False\n",
      "number of parameters:  992103\n",
      "encoder:  rnn\n",
      "hidden_size :  400\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  False\n",
      "number of parameters:  1806303\n",
      "encoder:  cnn\n",
      "hidden_size :  400\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  False\n",
      "number of parameters:  962303\n",
      "encoder:  cnn\n",
      "hidden_size :  400\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  5\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  False\n",
      "number of parameters:  1522303\n",
      "encoder:  rnn\n",
      "hidden_size :  500\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  False\n",
      "number of parameters:  2557503\n",
      "encoder:  cnn\n",
      "hidden_size :  500\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  False\n",
      "number of parameters:  1352503\n",
      "encoder:  cnn\n",
      "hidden_size :  500\n",
      "hid_dim:  300\n",
      "is_concat? :  False\n",
      "kernel_size:  5\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  False\n",
      "number of parameters:  2152503\n",
      "encoder:  rnn\n",
      "hidden_size :  300\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  False\n",
      "number of parameters:  1265103\n",
      "encoder:  cnn\n",
      "hidden_size :  300\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  False\n",
      "number of parameters:  722103\n",
      "encoder:  cnn\n",
      "hidden_size :  300\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  5\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  False\n",
      "number of parameters:  1082103\n",
      "encoder:  rnn\n",
      "hidden_size :  400\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  False\n",
      "number of parameters:  1926303\n",
      "encoder:  cnn\n",
      "hidden_size :  400\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  False\n",
      "number of parameters:  1082303\n",
      "encoder:  cnn\n",
      "hidden_size :  400\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  5\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  False\n",
      "number of parameters:  1642303\n",
      "encoder:  rnn\n",
      "hidden_size :  500\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  False\n",
      "number of parameters:  2707503\n",
      "encoder:  cnn\n",
      "hidden_size :  500\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  3\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  False\n",
      "number of parameters:  1502503\n",
      "encoder:  cnn\n",
      "hidden_size :  500\n",
      "hid_dim:  300\n",
      "is_concat? :  True\n",
      "kernel_size:  5\n",
      "initial learning_rate:  0.004\n",
      "weight decay? :  True\n",
      "dropout? :  False\n",
      "number of parameters:  2302503\n"
     ]
    }
   ],
   "source": [
    "# First lets improve libraries that we are going to be used in this lab session\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "plt.switch_backend('agg')\n",
    "random.seed(134)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "# create the dictionary and all train tokens\n",
    "VOCAB_SIZE = 50000\n",
    "EMBED_SIZE = 300\n",
    "# load data\n",
    "snli_val_data_tokens = pkl.load(open(\"snli_val_data_tokens.p\", \"rb\"))\n",
    "snli_train_data_tokens = pkl.load(open(\"snli_train_data_tokens.p\", \"rb\"))\n",
    "all_train_tokens = pkl.load(open(\"all_train_tokens.p\", \"rb\"))\n",
    "embedding_dict = pkl.load(open(\"embedding_dict.p\", \"rb\"))\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    id2token = list(all_tokens)\n",
    "    token2id = dict(zip(all_tokens, range(2,2+len(all_tokens))))\n",
    "    id2token = ['<pad>', '<unk>']  + id2token\n",
    "    token2id['<pad>'] = PAD_IDX\n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data, token2id):\n",
    "    prem_indices_data = []\n",
    "    hyp_indices_data = []\n",
    "    target_indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "#         print(tokens[0])\n",
    "#         print(tokens[1])\n",
    "        prem_index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens[0]]\n",
    "        hyp_index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens[1]]\n",
    "        prem_indices_data.append(prem_index_list)\n",
    "        hyp_indices_data.append(hyp_index_list)\n",
    "        target_indices_data.append(tokens[2])\n",
    "    return prem_indices_data, hyp_indices_data, target_indices_data\n",
    "token2id, id2token = build_vocab(all_train_tokens)\n",
    "val_prem_data_indices, val_hyp_data_indices, val_target_data_indices = token2index_dataset(snli_val_data_tokens, token2id)\n",
    "train_prem_data_indices, train_hyp_data_indices, train_target_data_indices = token2index_dataset(snli_train_data_tokens, token2id)\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 50\n",
    "\n",
    "\n",
    "# encode data loader\n",
    "class EncodeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prem_data_list, hyp_data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens\n",
    "        @param target_list: list of newsgroup targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.prem_data_list = prem_data_list\n",
    "        self.hyp_data_list = hyp_data_list\n",
    "        self.target_list = target_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prem_data_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        prem_token_idx = self.prem_data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        hyp_token_idx = self.hyp_data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [prem_token_idx, hyp_token_idx, label]\n",
    "\n",
    "\n",
    "def encode_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    prem_data_list = []\n",
    "    hyp_data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    # print(\"collate batch: \", batch[0][0])\n",
    "    # batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        prem_padded_vec = np.pad(np.array(datum[0]),\n",
    "                                 pad_width=((0, MAX_SENTENCE_LENGTH - len(datum[0]))),\n",
    "                                 mode=\"constant\", constant_values=0)\n",
    "        hyp_padded_vec = np.pad(np.array(datum[1]),\n",
    "                                pad_width=((0, MAX_SENTENCE_LENGTH - len(datum[1]))),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        prem_data_list.append(prem_padded_vec)\n",
    "        hyp_data_list.append(hyp_padded_vec)\n",
    "    return [torch.from_numpy((np.array(prem_data_list))), torch.from_numpy(np.array(hyp_data_list)),\n",
    "            torch.LongTensor(label_list)]\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_dataset = EncodeDataset(train_prem_data_indices, train_hyp_data_indices, train_target_data_indices)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=encode_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = EncodeDataset(val_prem_data_indices, val_hyp_data_indices, val_target_data_indices)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=encode_collate_func,\n",
    "                                           shuffle=True)\n",
    "# CNN Encoder\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, vocab_size, kernel_size, hid_dim, is_concat, is_dropout):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(np.array(embedding_dict).copy()))\n",
    "        # self.embedding.weight.requires_grad = False\n",
    "        if kernel_size == 3:\n",
    "            self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size, padding=1)\n",
    "            self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size, padding=1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size, padding=2)\n",
    "            self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size, padding=2)\n",
    "        if is_concat:\n",
    "            self.linear1 = nn.Linear(hidden_size*2, hid_dim)\n",
    "        else:\n",
    "            self.linear1 = nn.Linear(hidden_size, hid_dim)\n",
    "        self.linear2 = nn.Linear(hid_dim, 3)\n",
    "        self.is_concat = is_concat\n",
    "        self.is_dropout = is_dropout\n",
    "        if self.is_dropout == True:\n",
    "            self.dropout = nn.Dropout(0.5)\n",
    "    def encode(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        embed = self.embedding(x)\n",
    "        m = (x == 1)\n",
    "        m = m.unsqueeze(2).repeat(1, 1, EMBED_SIZE).type(torch.cuda.FloatTensor)\n",
    "        embed = m * embed + (1-m) * embed.clone().detach()\n",
    "        hidden = self.conv1(embed.transpose(1,2)).transpose(1,2)\n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, seq_len, hidden.size(-1))\n",
    "        hidden = self.conv2(hidden.transpose(1,2)).transpose(1,2)\n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, seq_len, hidden.size(-1))\n",
    "        hidden = torch.max(hidden, 1)[0]\n",
    "        return hidden\n",
    "    def forward(self, prem, hyp):\n",
    "        batch_size, seq_len = prem.size()\n",
    "        # encode premise\n",
    "        prem_code = self.encode(prem)\n",
    "        # encode hypothesis\n",
    "        hyp_code = self.encode(hyp)\n",
    "        # concat or multiply\n",
    "        if self.is_concat:\n",
    "            code = torch.cat((prem_code,hyp_code), dim=1)\n",
    "        else:\n",
    "            code = prem_code * hyp_code\n",
    "        code = self.linear1(code)\n",
    "        if self.is_dropout:\n",
    "            code = self.dropout(code)\n",
    "        code = self.linear2(code)\n",
    "        return code\n",
    "\n",
    "# RNN Encoder\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, vocab_size, hid_dim, is_concat, is_dropout):\n",
    "\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(np.array(embedding_dict).copy()))\n",
    "        self.bi_gru = nn.GRU(emb_size, hidden_size, num_layers=1, batch_first=True,bidirectional=True)\n",
    "        self.linear2 = nn.Linear(hid_dim, 3)\n",
    "        self.is_concat = is_concat\n",
    "        if is_concat:\n",
    "            self.linear1 = nn.Linear(hidden_size*2, hid_dim)\n",
    "        else:\n",
    "            self.linear1 = nn.Linear(hidden_size*1, hid_dim)\n",
    "        self.is_dropout = is_dropout\n",
    "        if self.is_dropout == True:\n",
    "            self.dropout = nn.Dropout(0.5)\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.zeros(2, batch_size, self.hidden_size).to(device)\n",
    "\n",
    "        return hidden\n",
    "    def encode(self, x):\n",
    "        # lengths = MAX_SENTENCE_LENGTH - x.eq(0).long().sum(1).squeeze()\n",
    "        # _, idx_sort = torch.sort(lengths, dim=0, descending=True)\n",
    "        # _, idx_unsort = torch.sort(idx_sort, dim=0)\n",
    "        # lengths = lengths[idx_sort]\n",
    "        # x = x.index_select(0, idx_sort)\n",
    "        batch_size, seq_len = x.size()\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        embed = self.embedding(x)\n",
    "        m = (x == 1)\n",
    "        m = m.unsqueeze(2).repeat(1, 1, EMBED_SIZE).type(torch.cuda.FloatTensor)\n",
    "        embed = m * embed + (1-m) * embed.clone().detach()\n",
    "        # embed = torch.nn.utils.rnn.pack_padded_sequence(embed, lengths.cpu().numpy(), batch_first=True)\n",
    "        output, hidden = self.bi_gru(embed, self.hidden)\n",
    "        hidden = torch.sum(hidden, dim = 0)\n",
    "        # hidden = hidden.index_select(0, idx_unsort)\n",
    "        return hidden\n",
    "    def forward(self, prem, hyp):\n",
    "        batch_size, seq_len = prem.size()\n",
    "        # encode premise\n",
    "        prem_code = self.encode(prem)\n",
    "        # encode hypothesis\n",
    "        hyp_code = self.encode(hyp)\n",
    "        # concat or multiply\n",
    "        if self.is_concat:\n",
    "            code = torch.cat((prem_code,hyp_code), dim=1)\n",
    "        else:\n",
    "            code = prem_code * hyp_code\n",
    "        code = self.linear1(code)\n",
    "        if self.is_dropout:\n",
    "            code = self.dropout(code)\n",
    "        code = self.linear2(code)\n",
    "        return code\n",
    "# Function for testing the model\n",
    "def test_model(loader, model, criterion):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    losses = 0\n",
    "    total = 0.0\n",
    "    model.eval()\n",
    "    for prem_data, hyp_data, labels in loader:\n",
    "        prem_data_batch, hyp_data_batch, label_batch = prem_data.to(device), hyp_data.to(device),labels.to(device)\n",
    "        outputs = F.softmax(model(prem_data_batch, hyp_data_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(label_batch.view_as(predicted)).sum().item()\n",
    "        losses += loss.item()\n",
    "    return (100 * correct / total), losses / total\n",
    "\n",
    "\n",
    "def plot_func(train_accs, val_accs, filename):\n",
    "    f = plt.figure()\n",
    "    plt.plot(train_accs, label='train');\n",
    "    plt.plot(val_accs, label='val');\n",
    "    plt.title(filename);\n",
    "    plt.legend()\n",
    "\n",
    "    f.savefig(filename + \".pdf\", bbox_inches='tight')\n",
    "\n",
    "def get_n_params(model):\n",
    "    pp = 0\n",
    "    for p in list(model.parameters()):\n",
    "        nn = 1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn * s\n",
    "        print(nn)\n",
    "        pp += nn\n",
    "    return pp\n",
    "    # plt.show()\n",
    "# Training\n",
    "# Train and valid function\n",
    "# hidden_size hidden size in cnn/rnn; hid_dim hidden dimension in fully connected network\n",
    "# encoder: 'cnn', 'rnn'\n",
    "def train_valid(encoder='cnn', hidden_size=200, hid_dim=200, is_concat=True, lr=0.01, is_wd=True,\n",
    "                is_dropout=True, kernel_size=3):\n",
    "    print('encoder: ', encoder)\n",
    "    print('hidden_size : ', hidden_size)\n",
    "    print('hid_dim: ', hid_dim)\n",
    "    print('is_concat? : ', is_concat)\n",
    "    print('kernel_size: ', kernel_size)\n",
    "    print('initial learning_rate: ', lr)\n",
    "    print('weight decay? : ', is_wd)\n",
    "    print('dropout? : ', is_dropout)\n",
    "    best_val_acc = 0\n",
    "    sys.stdout.flush()\n",
    "    filename = '_'.join([encoder, str(hidden_size),\n",
    "                         str(hid_dim), str(is_concat), str(kernel_size), str(is_wd), str(is_dropout)])\n",
    "    filename_acc = 'acc_'+filename\n",
    "    filename_loss = 'loss_'+filename\n",
    "    if encoder == 'cnn':\n",
    "        model = CNN(EMBED_SIZE, hidden_size, VOCAB_SIZE + 2, kernel_size, hid_dim, is_concat, is_dropout)\n",
    "    else:\n",
    "        model = RNN(EMBED_SIZE, hidden_size, VOCAB_SIZE + 2, hid_dim, is_concat, is_dropout)\n",
    "    model = model.to(device)\n",
    "    learning_rate = lr\n",
    "    num_epochs = 1  # number epoch to train\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    if is_wd:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    num_of_params = get_n_params(model) - 50001*300\n",
    "    print('number of parameters: ', num_of_params)\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # for epoch in range(num_epochs):\n",
    "    #     optimizer.defaults['lr'] = learning_rate/(epoch+1)\n",
    "    #     for i, (prem_data, hyp_data, labels) in enumerate(train_loader):\n",
    "    #         sys.stdout.flush()\n",
    "    #         model.train()\n",
    "    #         prem_data_batch, hyp_data_batch, label_batch = prem_data.to(device), hyp_data.to(device), labels.to(device)\n",
    "    #         optimizer.zero_grad()\n",
    "    #         outputs = model(prem_data_batch, hyp_data_batch)\n",
    "    #         loss = criterion(outputs, label_batch)\n",
    "    #         loss.backward()\n",
    "    #         optimizer.step()\n",
    "    #         # validate every 10 iterations\n",
    "    #         if i > 0 and i % 100 == 0:\n",
    "    #             # validate\n",
    "    #             val_acc, val_loss= test_model(val_loader, model, criterion)\n",
    "    #             train_acc, train_loss = test_model(train_loader, model, criterion)\n",
    "    #             val_accs.append(val_acc)\n",
    "    #             train_accs.append(train_acc)\n",
    "    #             train_losses.append(train_loss)\n",
    "    #             val_losses.append(val_loss)\n",
    "    #             print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {} Train Acc: {}'.format(\n",
    "    #                 epoch + 1, num_epochs, i + 1, len(train_loader), val_acc, train_acc))\n",
    "    #             sys.stdout.flush()\n",
    "    #             if best_val_acc < val_acc:\n",
    "    #                 best_val_acc = val_acc\n",
    "    #                 torch.save(model.state_dict(),\n",
    "    #                            filename+'.pth')\n",
    "    #             break\n",
    "    # plot_func(train_accs, val_accs, filename_acc)\n",
    "    # plot_func(train_losses, val_losses, filename_loss)\n",
    "    # print (\"After training for {} epochs\".format(num_epochs))\n",
    "    # print (\"Val Acc {}\".format(best_val_acc))\n",
    "    # sys.stdout.flush()\n",
    "    return best_val_acc\n",
    "\n",
    "# CNN\n",
    "# hidden size\n",
    "hidden_sizes = [300, 400, 500]\n",
    "encoders = ['rnn', 'cnn']\n",
    "is_wds = [False, True]\n",
    "is_dropouts = [True, False]\n",
    "is_concats = [False, True]\n",
    "kernel_sizes = [3, 5]\n",
    "best_acc = 0\n",
    "lr = 4e-3\n",
    "hid_dim = 300\n",
    "best_kernel_size = 3\n",
    "for is_wd in is_wds:\n",
    "    for is_dropout in is_dropouts:\n",
    "        for is_concat in is_concats:\n",
    "            for hidden_size in hidden_sizes:\n",
    "                for encoder in encoders:\n",
    "                    if encoder == 'rnn':\n",
    "                        acc =  train_valid(encoder, hidden_size, hid_dim, is_concat, lr, is_wd, is_dropout)\n",
    "                    else:\n",
    "                        for kernel_size in kernel_sizes:\n",
    "                            acc = train_valid(encoder, hidden_size, hid_dim, is_concat, lr, is_wd, is_dropout, kernel_size)\n",
    "sys.stdout.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
