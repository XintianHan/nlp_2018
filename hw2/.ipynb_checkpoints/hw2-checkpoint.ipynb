{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets improve libraries that we are going to be used in this lab session\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "import csv\n",
    "random.seed(134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '/Users/xintianhan/Downloads/nlp/hw2_data/'\n",
    "dirc_snli_train = directory + 'snli_train.tsv'\n",
    "dirc_snli_val = directory + 'snli_val.tsv'\n",
    "dirc_mnli_train = directory + 'mnli_train.tsv'\n",
    "dirc_mnli_val = directory + 'mnli_val.tsv'\n",
    "dirc_dict = directory + 'wiki-news-300d-1M.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get integer label from text input: 0, contradiction; 1, entailment; 2, neutral\n",
    "def get_label(sent):\n",
    "    if sent == 'contradiction':\n",
    "        return 0\n",
    "    elif sent == 'entailment':\n",
    "        return 1\n",
    "    elif sent == 'neutral':\n",
    "        return 2\n",
    "    else:\n",
    "        print('invalid input!')\n",
    "def tokenize_dataset(dirc):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    flag = 0\n",
    "    with open(dirc) as tsvfile:\n",
    "        reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            # skip the first line by flag\n",
    "            if flag == 0:\n",
    "                flag = 1\n",
    "                continue\n",
    "            prem = row[0].split()\n",
    "            hyp = row[1].split()\n",
    "            label = get_label(row[2])\n",
    "            token_dataset.append([prem,hyp,label])\n",
    "    return token_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing SNLI val data\n",
      "Tokenizing SNLI train data\n",
      "Tokenizing MNLI val data\n",
      "Tokenizing MNLI train data\n"
     ]
    }
   ],
   "source": [
    "# SNLI val set tokens\n",
    "print (\"Tokenizing SNLI val data\")\n",
    "snli_val_data_tokens = tokenize_dataset(dirc_snli_val)\n",
    "pkl.dump(snli_val_data_tokens, open(\"snli_val_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# SNLI train set tokens\n",
    "print (\"Tokenizing SNLI train data\")\n",
    "snli_train_data_tokens = tokenize_dataset(dirc_snli_train)\n",
    "pkl.dump(snli_train_data_tokens, open(\"snli_train_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# MNLI val set tokens\n",
    "print (\"Tokenizing MNLI val data\")\n",
    "mnli_val_data_tokens = tokenize_dataset(dirc_mnli_val)\n",
    "pkl.dump(mnli_val_data_tokens, open(\"mnli_val_data_tokens.p\", \"wb\"))\n",
    "\n",
    "\n",
    "# MNLI train set tokens\n",
    "print (\"Tokenizing MNLI train data\")\n",
    "mnli_train_data_tokens = tokenize_dataset(dirc_mnli_train)\n",
    "pkl.dump(mnli_train_data_tokens, open(\"mnli_train_data_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dictionary and all train tokens\n",
    "VOCAB_SIZE = 50000\n",
    "EMBED_SIZE = 300\n",
    "import io\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    embedding_dict = np.zeros((VOCAB_SIZE+2, EMBED_SIZE))\n",
    "    all_train_tokens = []\n",
    "    i = 0\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        all_train_tokens.append(tokens[0])\n",
    "        embedding_dict[i+2] = list(map(float, tokens[1:]))\n",
    "        i += 1\n",
    "        if i == VOCAB_SIZE:\n",
    "            break\n",
    "    return embedding_dict, all_train_tokens\n",
    "embedding_dict, all_train_tokens = load_vectors(dirc_dict)\n",
    "pkl.dump(all_train_tokens, open('all_train_tokens.p', \"wb\"))\n",
    "pkl.dump(embedding_dict, open('embedding_dict.p',\"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
