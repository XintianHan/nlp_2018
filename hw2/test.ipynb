{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "EMBED_SIZE = 300\n",
    "VOCAB_SIZE = 50000\n",
    "PAD_IDX = 0\n",
    "# RNN Encoder\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, vocab_size, hid_dim, is_concat, is_dropout):\n",
    "\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(np.array(embedding_dict).copy()))\n",
    "        self.bi_gru = nn.GRU(emb_size, hidden_size, num_layers=1, batch_first=True,bidirectional=True)\n",
    "        self.linear2 = nn.Linear(hid_dim, 3)\n",
    "        self.is_concat = is_concat\n",
    "        if is_concat:\n",
    "            self.linear1 = nn.Linear(hidden_size*2, hid_dim)\n",
    "        else:\n",
    "            self.linear1 = nn.Linear(hidden_size*1, hid_dim)\n",
    "        self.is_dropout = is_dropout\n",
    "        if self.is_dropout == True:\n",
    "            self.dropout = nn.Dropout(0.5)\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.zeros(2, batch_size, self.hidden_size).to(device)\n",
    "\n",
    "        return hidden\n",
    "    def encode(self, x):\n",
    "        # lengths = MAX_SENTENCE_LENGTH - x.eq(0).long().sum(1).squeeze()\n",
    "        # _, idx_sort = torch.sort(lengths, dim=0, descending=True)\n",
    "        # _, idx_unsort = torch.sort(idx_sort, dim=0)\n",
    "        # lengths = lengths[idx_sort]\n",
    "        # x = x.index_select(0, idx_sort)\n",
    "        batch_size, seq_len = x.size()\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        embed = self.embedding(x)\n",
    "        m = (x == 1)\n",
    "        m = m.unsqueeze(2).repeat(1, 1, EMBED_SIZE).type(torch.FloatTensor).to(device)\n",
    "        embed = m * embed + (1-m) * embed.clone().detach()\n",
    "        # embed = torch.nn.utils.rnn.pack_padded_sequence(embed, lengths.cpu().numpy(), batch_first=True)\n",
    "        output, hidden = self.bi_gru(embed, self.hidden)\n",
    "        hidden = torch.sum(hidden, dim = 0)\n",
    "        # hidden = hidden.index_select(0, idx_unsort)\n",
    "        return hidden\n",
    "    def forward(self, prem, hyp):\n",
    "        batch_size, seq_len = prem.size()\n",
    "        # encode premise\n",
    "        prem_code = self.encode(prem)\n",
    "        # encode hypothesis\n",
    "        hyp_code = self.encode(hyp)\n",
    "        # concat or multiply\n",
    "        if self.is_concat:\n",
    "            code = torch.cat((prem_code,hyp_code), dim=1)\n",
    "        else:\n",
    "            code = prem_code * hyp_code\n",
    "        code = self.linear1(code)\n",
    "        if self.is_dropout:\n",
    "            code = self.dropout(code)\n",
    "        code = F.relu(code)\n",
    "        code = self.linear2(code)\n",
    "        return code\n",
    "snli_val_data_tokens = pkl.load(open(\"snli_val_data_tokens.p\", \"rb\"))\n",
    "snli_train_data_tokens = pkl.load(open(\"snli_train_data_tokens.p\", \"rb\"))\n",
    "all_train_tokens = pkl.load(open(\"all_train_tokens.p\", \"rb\"))\n",
    "embedding_dict = pkl.load(open(\"embedding_dict.p\", \"rb\"))\n",
    "hidden_size = 400\n",
    "hid_dim = 300\n",
    "is_concat = False\n",
    "is_wd = False\n",
    "is_dropout = True\n",
    "kernel_size = 3\n",
    "model = RNN(EMBED_SIZE, hidden_size, VOCAB_SIZE + 2, hid_dim, is_concat, is_dropout)\n",
    "model.load_state_dict(torch.load('rnn_400_300_False_3_False_True.pth', map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dictionary and all train tokens\n",
    "VOCAB_SIZE = 50000\n",
    "EMBED_SIZE = 300\n",
    "# load data\n",
    "snli_val_data_tokens = pkl.load(open(\"snli_val_data_tokens.p\", \"rb\"))\n",
    "snli_train_data_tokens = pkl.load(open(\"snli_train_data_tokens.p\", \"rb\"))\n",
    "all_train_tokens = pkl.load(open(\"all_train_tokens.p\", \"rb\"))\n",
    "embedding_dict = pkl.load(open(\"embedding_dict.p\", \"rb\"))\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "BATCH_SIZE = 100\n",
    "# encode data loader\n",
    "class EncodeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prem_data_list, hyp_data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens\n",
    "        @param target_list: list of newsgroup targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.prem_data_list = prem_data_list\n",
    "        self.hyp_data_list = hyp_data_list\n",
    "        self.target_list = target_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prem_data_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        prem_token_idx = self.prem_data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        hyp_token_idx = self.hyp_data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [prem_token_idx, hyp_token_idx, label]\n",
    "\n",
    "\n",
    "def encode_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    prem_data_list = []\n",
    "    hyp_data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    # print(\"collate batch: \", batch[0][0])\n",
    "    # batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        prem_padded_vec = np.pad(np.array(datum[0]),\n",
    "                                 pad_width=((0, MAX_SENTENCE_LENGTH - len(datum[0]))),\n",
    "                                 mode=\"constant\", constant_values=0)\n",
    "        hyp_padded_vec = np.pad(np.array(datum[1]),\n",
    "                                pad_width=((0, MAX_SENTENCE_LENGTH - len(datum[1]))),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        prem_data_list.append(prem_padded_vec)\n",
    "        hyp_data_list.append(hyp_padded_vec)\n",
    "    return [torch.from_numpy((np.array(prem_data_list))), torch.from_numpy(np.array(hyp_data_list)),\n",
    "            torch.LongTensor(label_list)]\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    id2token = list(all_tokens)\n",
    "    token2id = dict(zip(all_tokens, range(2,2+len(all_tokens))))\n",
    "    id2token = ['<pad>', '<unk>']  + id2token\n",
    "    token2id['<pad>'] = PAD_IDX\n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data, token2id):\n",
    "    prem_indices_data = []\n",
    "    hyp_indices_data = []\n",
    "    target_indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "#         print(tokens[0])\n",
    "#         print(tokens[1])\n",
    "        prem_index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens[0]]\n",
    "        hyp_index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens[1]]\n",
    "        prem_indices_data.append(prem_index_list)\n",
    "        hyp_indices_data.append(hyp_index_list)\n",
    "        target_indices_data.append(tokens[2])\n",
    "    return prem_indices_data, hyp_indices_data, target_indices_data\n",
    "token2id, id2token = build_vocab(all_train_tokens)\n",
    "val_prem_data_indices, val_hyp_data_indices, val_target_data_indices = token2index_dataset(snli_val_data_tokens, token2id)\n",
    "\n",
    "val_dataset = EncodeDataset(val_prem_data_indices, val_hyp_data_indices, val_target_data_indices)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=encode_collate_func,\n",
    "                                           shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1, 1, 1, 0, 1, 2, 0, 2, 1, 0, 2, 0, 1, 1, 2, 2, 1, 2, 1, 2, 1, 0,\n",
      "        0, 2, 1, 0, 1, 0, 0, 2, 1, 0, 0, 0, 1, 0, 0, 0, 2, 1, 1, 0, 2, 2, 1, 1,\n",
      "        2, 2, 2, 1, 2, 1, 0, 2, 1, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 1, 1, 2, 0, 0,\n",
      "        1, 0, 2, 0, 1, 1, 1, 0, 2, 1, 1, 2, 0, 1, 1, 0, 1, 2, 0, 0, 0, 0, 2, 2,\n",
      "        0, 1, 2, 1])\n",
      "tensor([0, 1, 1, 1, 1, 1, 1, 0, 2, 0, 1, 1, 2, 0, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1,\n",
      "        0, 2, 0, 1, 1, 2, 2, 2, 0, 2, 0, 0, 2, 0, 1, 0, 0, 0, 1, 1, 2, 2, 1, 1,\n",
      "        2, 0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 2, 1, 0, 2, 2,\n",
      "        1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 1, 1, 2, 0, 2, 1, 1, 0, 1, 0, 2, 0, 2, 0,\n",
      "        0, 1, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "MAX_SENTENCE_LENGTH = 50\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "for i, (prem_data, hyp_data, labels) in enumerate(val_loader):\n",
    "    outputs = F.softmax(model(prem_data, hyp_data), dim=1)\n",
    "    predicted = outputs.max(1, keepdim=True)[1]\n",
    "    print(predicted.view_as(labels))\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct predictions\n",
    "# Read Doc\n",
    "directory = '/Users/xintianhan/Downloads/nlp/hw2_data/'\n",
    "dirc_snli_train = directory + 'snli_train.tsv'\n",
    "dirc_snli_val = directory + 'snli_val.tsv'\n",
    "dirc_mnli_train = directory + 'mnli_train.tsv'\n",
    "dirc_mnli_val = directory + 'mnli_val.tsv'\n",
    "dirc_dict = directory + 'wiki-news-300d-1M.vec'\n",
    "def get_label(sent):\n",
    "    if sent == 'contradiction':\n",
    "        return 0\n",
    "    elif sent == 'entailment':\n",
    "        return 1\n",
    "    elif sent == 'neutral':\n",
    "        return 2\n",
    "    else:\n",
    "        print('invalid input!')\n",
    "def tokenize_dataset(dirc):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    flag = 0\n",
    "    with open(dirc) as tsvfile:\n",
    "        reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            # skip the first line by flag\n",
    "            if flag == 0:\n",
    "                flag = 1\n",
    "                continue\n",
    "            prem = row[0].split()\n",
    "            hyp = row[1].split()\n",
    "            label = get_label(row[2])\n",
    "            token_dataset.append([prem,hyp,label])\n",
    "    return token_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_val_data_tokens = tokenize_dataset(dirc_snli_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_val_data_tokens = tokenize_dataset(dirc_mnli_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = 0\n",
    "genes = []\n",
    "with open(dirc_mnli_val) as tsvfile:\n",
    "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "    for row in reader:\n",
    "        # skip the first line by flag\n",
    "        if flag == 0:\n",
    "            flag = 1\n",
    "            continue\n",
    "        gene = row[3]\n",
    "        genes.append(gene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genes = []\n",
    "for item in genes:\n",
    "    if item not in all_genes:\n",
    "        all_genes.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fiction', 'telephone', 'slate', 'government', 'travel']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_val_data_tokens = pkl.load(open(\"mnli_val_data_tokens.p\", \"rb\"))\n",
    "token2id, id2token = build_vocab(all_train_tokens)\n",
    "val_prem_data_indices, val_hyp_data_indices, val_target_data_indices = token2index_dataset(mnli_val_data_tokens, token2id)\n",
    "val_prem_data_indices = np.array(val_prem_data_indices)\n",
    "val_hyp_data_indices = np.array(val_hyp_data_indices)\n",
    "val_target_data_indices = np.array(val_target_data_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiction\n",
      "995\n",
      "41.20603015075377\n",
      "telephone\n",
      "1005\n",
      "41.99004975124378\n",
      "slate\n",
      "1002\n",
      "38.12375249500998\n",
      "government\n",
      "1016\n",
      "39.468503937007874\n",
      "travel\n",
      "982\n",
      "39.71486761710794\n"
     ]
    }
   ],
   "source": [
    "def test_model(loader, model, criterion):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    losses = 0\n",
    "    total = 0.0\n",
    "    model.eval()\n",
    "    for prem_data, hyp_data, labels in loader:\n",
    "        prem_data_batch, hyp_data_batch, label_batch = prem_data.to(device), hyp_data.to(device),labels.to(device)\n",
    "        outputs = F.softmax(model(prem_data_batch, hyp_data_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(label_batch.view_as(predicted)).sum().item()\n",
    "        losses += loss.item()\n",
    "    return (100 * correct / total), losses / total\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "for i in range(len(all_genes)):\n",
    "    print(all_genes[i])\n",
    "    cur_val_prem_data_indices = val_prem_data_indices[genes == all_genes[i]]\n",
    "    cur_val_hyp_data_indices = val_hyp_data_indices[genes == all_genes[i]]\n",
    "    cur_val_target_data_indices = val_target_data_indices[genes == all_genes[i]]\n",
    "    print(len(cur_val_prem_data_indices))\n",
    "    val_dataset = EncodeDataset(cur_val_prem_data_indices, cur_val_hyp_data_indices, cur_val_target_data_indices)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=encode_collate_func,\n",
    "                                               shuffle=False)\n",
    "    acc, loss = test_model(val_loader, model, criterion)\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.5\n"
     ]
    }
   ],
   "source": [
    "snli_val_data_tokens = pkl.load(open(\"snli_val_data_tokens.p\", \"rb\"))\n",
    "token2id, id2token = build_vocab(all_train_tokens)\n",
    "val_prem_data_indices, val_hyp_data_indices, val_target_data_indices = token2index_dataset(snli_val_data_tokens, token2id)\n",
    "def test_model(loader, model, criterion):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    losses = 0\n",
    "    total = 0.0\n",
    "    model.eval()\n",
    "    for prem_data, hyp_data, labels in loader:\n",
    "        prem_data_batch, hyp_data_batch, label_batch = prem_data.to(device), hyp_data.to(device),labels.to(device)\n",
    "        outputs = F.softmax(model(prem_data_batch, hyp_data_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(label_batch.view_as(predicted)).sum().item()\n",
    "        losses += loss.item()\n",
    "    return (100 * correct / total), losses / total\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "val_dataset = EncodeDataset(val_prem_data_indices, val_hyp_data_indices, val_target_data_indices)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=encode_collate_func,\n",
    "                                           shuffle=False)\n",
    "acc, loss = test_model(val_loader, model, criterion)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Encoder\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, vocab_size, kernel_size, hid_dim, is_concat, is_dropout):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(np.array(embedding_dict).copy()))\n",
    "        # self.embedding.weight.requires_grad = False\n",
    "        if kernel_size == 3:\n",
    "            self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size, padding=1)\n",
    "            self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size, padding=1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size, padding=2)\n",
    "            self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size, padding=2)\n",
    "        if is_concat:\n",
    "            self.linear1 = nn.Linear(hidden_size*2, hid_dim)\n",
    "        else:\n",
    "            self.linear1 = nn.Linear(hidden_size, hid_dim)\n",
    "        self.linear2 = nn.Linear(hid_dim, 3)\n",
    "        self.is_concat = is_concat\n",
    "        self.is_dropout = is_dropout\n",
    "        if self.is_dropout == True:\n",
    "            self.dropout = nn.Dropout(0.5)\n",
    "    def encode(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        embed = self.embedding(x)\n",
    "        m = (x == 1)\n",
    "        m = m.unsqueeze(2).repeat(1, 1, EMBED_SIZE).type(torch.FloatTensor).to(device)\n",
    "        embed = m * embed + (1-m) * embed.clone().detach()\n",
    "        hidden = self.conv1(embed.transpose(1,2)).transpose(1,2)\n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, seq_len, hidden.size(-1))\n",
    "        hidden = self.conv2(hidden.transpose(1,2)).transpose(1,2)\n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, seq_len, hidden.size(-1))\n",
    "        hidden = torch.max(hidden, 1)[0]\n",
    "        return hidden\n",
    "    def forward(self, prem, hyp):\n",
    "        batch_size, seq_len = prem.size()\n",
    "        # encode premise\n",
    "        prem_code = self.encode(prem)\n",
    "        # encode hypothesis\n",
    "        hyp_code = self.encode(hyp)\n",
    "        # concat or multiply\n",
    "        if self.is_concat:\n",
    "            code = torch.cat((prem_code,hyp_code), dim=1)\n",
    "        else:\n",
    "            code = prem_code * hyp_code\n",
    "        code = self.linear1(code)\n",
    "        code = F.relu(code)\n",
    "        if self.is_dropout:\n",
    "            code = self.dropout(code)\n",
    "        code = self.linear2(code)\n",
    "        return code\n",
    "snli_val_data_tokens = pkl.load(open(\"snli_val_data_tokens.p\", \"rb\"))\n",
    "snli_train_data_tokens = pkl.load(open(\"snli_train_data_tokens.p\", \"rb\"))\n",
    "all_train_tokens = pkl.load(open(\"all_train_tokens.p\", \"rb\"))\n",
    "embedding_dict = pkl.load(open(\"embedding_dict.p\", \"rb\"))\n",
    "hidden_size = 300\n",
    "hid_dim = 300\n",
    "is_concat = False\n",
    "is_wd = False\n",
    "is_dropout = True\n",
    "kernel_size = 3\n",
    "model = CNN(EMBED_SIZE, hidden_size, VOCAB_SIZE + 2, kernel_size, hid_dim, is_concat, is_dropout)\n",
    "model.load_state_dict(torch.load('cnn_300_300_False_3_False_True.pth', map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.2\n"
     ]
    }
   ],
   "source": [
    "snli_val_data_tokens = pkl.load(open(\"snli_val_data_tokens.p\", \"rb\"))\n",
    "token2id, id2token = build_vocab(all_train_tokens)\n",
    "val_prem_data_indices, val_hyp_data_indices, val_target_data_indices = token2index_dataset(snli_val_data_tokens, token2id)\n",
    "def test_model(loader, model, criterion):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    losses = 0\n",
    "    total = 0.0\n",
    "    model.eval()\n",
    "    for prem_data, hyp_data, labels in loader:\n",
    "        prem_data_batch, hyp_data_batch, label_batch = prem_data.to(device), hyp_data.to(device),labels.to(device)\n",
    "        outputs = F.softmax(model(prem_data_batch, hyp_data_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(label_batch.view_as(predicted)).sum().item()\n",
    "        losses += loss.item()\n",
    "    return (100 * correct / total), losses / total\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "val_dataset = EncodeDataset(val_prem_data_indices, val_hyp_data_indices, val_target_data_indices)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=encode_collate_func,\n",
    "                                           shuffle=False)\n",
    "acc, loss = test_model(val_loader, model, criterion)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiction\n",
      "995\n",
      "39.19597989949749\n",
      "telephone\n",
      "1005\n",
      "37.21393034825871\n",
      "slate\n",
      "1002\n",
      "39.52095808383233\n",
      "government\n",
      "1016\n",
      "37.59842519685039\n",
      "travel\n",
      "982\n",
      "39.0020366598778\n"
     ]
    }
   ],
   "source": [
    "mnli_val_data_tokens = pkl.load(open(\"mnli_val_data_tokens.p\", \"rb\"))\n",
    "token2id, id2token = build_vocab(all_train_tokens)\n",
    "val_prem_data_indices, val_hyp_data_indices, val_target_data_indices = token2index_dataset(mnli_val_data_tokens, token2id)\n",
    "val_prem_data_indices = np.array(val_prem_data_indices)\n",
    "val_hyp_data_indices = np.array(val_hyp_data_indices)\n",
    "val_target_data_indices = np.array(val_target_data_indices)\n",
    "def test_model(loader, model, criterion):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    losses = 0\n",
    "    total = 0.0\n",
    "    model.eval()\n",
    "    for prem_data, hyp_data, labels in loader:\n",
    "        prem_data_batch, hyp_data_batch, label_batch = prem_data.to(device), hyp_data.to(device),labels.to(device)\n",
    "        outputs = F.softmax(model(prem_data_batch, hyp_data_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(label_batch.view_as(predicted)).sum().item()\n",
    "        losses += loss.item()\n",
    "    return (100 * correct / total), losses / total\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "for i in range(len(all_genes)):\n",
    "    print(all_genes[i])\n",
    "    cur_val_prem_data_indices = val_prem_data_indices[genes == all_genes[i]]\n",
    "    cur_val_hyp_data_indices = val_hyp_data_indices[genes == all_genes[i]]\n",
    "    cur_val_target_data_indices = val_target_data_indices[genes == all_genes[i]]\n",
    "    print(len(cur_val_prem_data_indices))\n",
    "    val_dataset = EncodeDataset(cur_val_prem_data_indices, cur_val_hyp_data_indices, cur_val_target_data_indices)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=encode_collate_func,\n",
    "                                               shuffle=False)\n",
    "    acc, loss = test_model(val_loader, model, criterion)\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = 0\n",
    "train_genes = []\n",
    "with open(dirc_mnli_train) as tsvfile:\n",
    "    reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "    for row in reader:\n",
    "        # skip the first line by flag\n",
    "        if flag == 0:\n",
    "            flag = 1\n",
    "            continue\n",
    "        gene = row[3]\n",
    "        train_genes.append(gene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_genes = np.array(train_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiction\n",
      "44.42211055276382\n",
      "42.814070351758794\n",
      "43.81909547738694\n",
      "44.42211055276382\n",
      "45.32663316582914\n",
      "telephone\n",
      "46.96517412935324\n",
      "48.25870646766169\n",
      "48.756218905472636\n",
      "49.850746268656714\n",
      "50.74626865671642\n",
      "slate\n",
      "39.62075848303393\n",
      "40.5189620758483\n",
      "40.7185628742515\n",
      "41.21756487025948\n",
      "43.712574850299404\n",
      "government\n",
      "49.01574803149607\n",
      "49.60629921259842\n",
      "50.39370078740158\n",
      "52.06692913385827\n",
      "53.346456692913385\n",
      "travel\n",
      "47.759674134419555\n",
      "48.16700610997963\n",
      "49.287169042769854\n",
      "49.08350305498982\n",
      "49.69450101832994\n"
     ]
    }
   ],
   "source": [
    "# Fine Tune\n",
    "mnli_train_data_tokens = pkl.load(open(\"mnli_train_data_tokens.p\", \"rb\"))\n",
    "token2id, id2token = build_vocab(all_train_tokens)\n",
    "train_prem_data_indices, train_hyp_data_indices, train_target_data_indices = token2index_dataset(mnli_train_data_tokens, token2id)\n",
    "train_prem_data_indices = np.array(train_prem_data_indices)\n",
    "train_hyp_data_indices = np.array(train_hyp_data_indices)\n",
    "train_target_data_indices = np.array(train_target_data_indices)\n",
    "for i in range(len(all_genes)):\n",
    "    print(all_genes[i])\n",
    "    model = RNN(EMBED_SIZE, hidden_size, VOCAB_SIZE + 2, hid_dim, is_concat, is_dropout)\n",
    "    model.load_state_dict(torch.load('rnn_400_300_False_3_False_True.pth', map_location=lambda storage, loc: storage))\n",
    "    learning_rate = 1e-4\n",
    "    num_epochs = 5 # number epoch to train\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    cur_train_prem_data_indices = train_prem_data_indices[train_genes == all_genes[i]]\n",
    "    cur_train_hyp_data_indices = train_hyp_data_indices[train_genes == all_genes[i]]\n",
    "    cur_train_target_data_indices = train_target_data_indices[train_genes == all_genes[i]]\n",
    "    train_dataset = EncodeDataset(cur_train_prem_data_indices, cur_train_hyp_data_indices, cur_train_target_data_indices)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=encode_collate_func,\n",
    "                                               shuffle=False)\n",
    "    cur_val_prem_data_indices = val_prem_data_indices[genes == all_genes[i]]\n",
    "    cur_val_hyp_data_indices = val_hyp_data_indices[genes == all_genes[i]]\n",
    "    cur_val_target_data_indices = val_target_data_indices[genes == all_genes[i]]\n",
    "    val_dataset = EncodeDataset(cur_val_prem_data_indices, cur_val_hyp_data_indices, cur_val_target_data_indices)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=encode_collate_func,\n",
    "                                               shuffle=False)\n",
    "    for epoch in range(num_epochs):\n",
    "        for j, (prem_data, hyp_data, labels) in enumerate(train_loader):\n",
    "            sys.stdout.flush()\n",
    "            model.train()\n",
    "            prem_data_batch, hyp_data_batch, label_batch = prem_data.to(device), hyp_data.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(prem_data_batch, hyp_data_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 10 iterations\n",
    "        val_acc, val_loss= test_model(val_loader, model, criterion)\n",
    "        train_acc, train_loss = test_model(train_loader, model, criterion)\n",
    "        print(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiction\n",
      "34.472361809045225\n",
      "34.472361809045225\n",
      "34.472361809045225\n",
      "34.472361809045225\n",
      "34.472361809045225\n",
      "telephone\n",
      "29.45273631840796\n",
      "36.417910447761194\n",
      "36.517412935323385\n",
      "36.11940298507463\n",
      "36.11940298507463\n",
      "slate\n",
      "30.139720558882235\n",
      "35.42914171656687\n",
      "35.728542914171655\n",
      "35.728542914171655\n",
      "35.92814371257485\n",
      "government\n",
      "36.71259842519685\n",
      "36.71259842519685\n",
      "32.97244094488189\n",
      "31.299212598425196\n",
      "29.921259842519685\n",
      "travel\n",
      "35.437881873727086\n",
      "35.437881873727086\n",
      "36.04887983706721\n",
      "35.74338085539715\n",
      "36.04887983706721\n"
     ]
    }
   ],
   "source": [
    "# Fine Tune\n",
    "mnli_train_data_tokens = pkl.load(open(\"mnli_train_data_tokens.p\", \"rb\"))\n",
    "token2id, id2token = build_vocab(all_train_tokens)\n",
    "train_prem_data_indices, train_hyp_data_indices, train_target_data_indices = token2index_dataset(mnli_train_data_tokens, token2id)\n",
    "train_prem_data_indices = np.array(train_prem_data_indices)\n",
    "train_hyp_data_indices = np.array(train_hyp_data_indices)\n",
    "train_target_data_indices = np.array(train_target_data_indices)\n",
    "for i in range(len(all_genes)):\n",
    "    print(all_genes[i])\n",
    "    model = RNN(EMBED_SIZE, hidden_size, VOCAB_SIZE + 2, hid_dim, is_concat, is_dropout)\n",
    "#     model.load_state_dict(torch.load('rnn_400_300_False_3_False_True.pth', map_location=lambda storage, loc: storage))\n",
    "    learning_rate = 1e-4\n",
    "    num_epochs = 5 # number epoch to train\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    cur_train_prem_data_indices = train_prem_data_indices[train_genes == all_genes[i]]\n",
    "    cur_train_hyp_data_indices = train_hyp_data_indices[train_genes == all_genes[i]]\n",
    "    cur_train_target_data_indices = train_target_data_indices[train_genes == all_genes[i]]\n",
    "    train_dataset = EncodeDataset(cur_train_prem_data_indices, cur_train_hyp_data_indices, cur_train_target_data_indices)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=encode_collate_func,\n",
    "                                               shuffle=False)\n",
    "    cur_val_prem_data_indices = val_prem_data_indices[genes == all_genes[i]]\n",
    "    cur_val_hyp_data_indices = val_hyp_data_indices[genes == all_genes[i]]\n",
    "    cur_val_target_data_indices = val_target_data_indices[genes == all_genes[i]]\n",
    "    val_dataset = EncodeDataset(cur_val_prem_data_indices, cur_val_hyp_data_indices, cur_val_target_data_indices)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=encode_collate_func,\n",
    "                                               shuffle=False)\n",
    "    for epoch in range(num_epochs):\n",
    "        for j, (prem_data, hyp_data, labels) in enumerate(train_loader):\n",
    "            sys.stdout.flush()\n",
    "            model.train()\n",
    "            prem_data_batch, hyp_data_batch, label_batch = prem_data.to(device), hyp_data.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(prem_data_batch, hyp_data_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 10 iterations\n",
    "        val_acc, val_loss= test_model(val_loader, model, criterion)\n",
    "        train_acc, train_loss = test_model(train_loader, model, criterion)\n",
    "        print(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
